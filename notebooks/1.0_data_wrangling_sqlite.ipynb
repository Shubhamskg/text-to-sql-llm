{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database - Schema Wrangling\n",
    "\n",
    "The spider database comes with some json files that have information, but I want to find a way to get into the dbs themselves to get the info in a more scaleable manner. In this notebook, I'll work through the .sqlite databases to build out the schema information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting watermark\n",
      "  Downloading watermark-2.4.3-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: ipython>=6.0 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from watermark) (8.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from watermark) (6.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\python311\\lib\\site-packages (from watermark) (65.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from importlib-metadata>=1.4->watermark) (3.17.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.0->watermark) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.0->watermark) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.0->watermark) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.0->watermark) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.0->watermark) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.0->watermark) (1.16.0)\n",
      "Installing collected packages: watermark\n",
      "Successfully installed watermark-2.4.3\n",
      "Last updated: 2023-11-02T11:39:19.036095+05:30\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.0\n",
      "IPython version      : 8.12.0\n",
      "\n",
      "Compiler    : MSC v.1933 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 141 Stepping 1, GenuineIntel\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install watermark\n",
    "from watermark import watermark\n",
    "print(watermark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy .sqlite files to common directory\n",
    "\n",
    "Use terminal commands to copy from data/raw/spider/database/ to data/processed/db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###commented out so we don't run again###\n",
    "#create a 'db' directory in the data/processed folder\n",
    "#! mkdir ../data/processed/db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###commented out so we don't run again###\n",
    "#run command to move all .sqlite folders to our newly created directory\n",
    "\n",
    "# src_dir = \"/Users/brettly/Sboard/projects/text-to-sql/data/raw/spider/database\"\n",
    "# dst_dir = \"/Users/brettly/Sboard/projects/text-to-sql/data/processed/db\"\n",
    "# for root, dirs, files in os.walk(src_dir):\n",
    "#     for f in files:\n",
    "#         if f.endswith('.sqlite'):\n",
    "#             shutil.copy(os.path.join(root,f), dst_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions For Getting Schema Info\n",
    "\n",
    "I already went through this with a really complicated script to get the details from a supplied json file. But that was with the itention of getting everything into a PostgreSQL database. Right now I want to prioritize the application and model itself rather than fussing with the data, so I'm going to pivot to using the supplies .sqlite files.\n",
    "\n",
    "So while I already have some schema info, it was really lacking in the supplied dtypes (only text and number), so I'm going to cleanup the process and build functions to extract the info directly from the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(filepath, filetype):\n",
    "    \"\"\"Create empty list, loop through files within a directory and grab those of a specifed filetype. Append those to the empty list and return without the filetypes.\n",
    "    filepath example: \"../data/processed/db/\"\n",
    "    filetype example: \".sqlite\"\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        for file in files:\n",
    "            if file.endswith(filetype):\n",
    "                file_list.append(os.path.join(file))\n",
    "\n",
    "    filenames = [file.replace(filetype, \"\") for file in file_list]\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names(db_path):\n",
    "    \"\"\"Function to use within build_schema_info to pull down table names from given schema, loop through them, and save to a list\"\"\"\n",
    "    \n",
    "    table_list = []\n",
    "\n",
    "    db = sqlite3.connect(db_path)\n",
    "    cursor=db.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    for table in tables:\n",
    "        table_list.append(table[0])\n",
    "    \n",
    "    cursor.close()\n",
    "    db.close()\n",
    "    \n",
    "    return table_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_info(db_path, schema_name, table_name):\n",
    "    \"\"\"Function to use within build_schema_info to pull the column info under the specific table_name\"\"\"\n",
    "    \n",
    "    db = sqlite3.connect(db_path)\n",
    "    cursor=db.cursor()\n",
    "    cursor.execute(\"PRAGMA table_info(\" + table_name + \");\")\n",
    "    columns = cursor.fetchall()\n",
    "\n",
    "    column_data = (schema_name, table_name, columns)\n",
    "\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "\n",
    "    return column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the processing and only leave the additional columns created\n",
    "def df_text_processing(df, text_columns=['schema', 'table', 'c_name']):\n",
    "    \"\"\"Function to apply some simple text processing to our schema info\"\"\"\n",
    "    #df[text_columns] = df[text_columns].apply(lambda x: x.str.lower()) #make lowercase\n",
    "    \n",
    "    #punc = string.punctuation\n",
    "    #df[text_columns] = df[text_columns].apply(lambda x: [word for word in x if word not in punc]) #remove punctuation\n",
    "\n",
    "    for col in text_columns:\n",
    "        df[col+'_split'] = df[col].str.replace('_', ' ') #loop through gext columns and create versions that breakout the '_' connected titles into seperate words\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_schema_info(filepath, filetype):\n",
    "    \"\"\"Function combines the get_filenames, get_table_names, and get_column_info funciton to create list of file names in a subdirectory and running the PRAGMA table info against each, saving their results to a list.\n",
    "    It then builds a pandas dataframe with the full schema info\"\"\"\n",
    "    \n",
    "    schema_list = get_filenames(filepath, filetype)\n",
    "\n",
    "    schema_data = []\n",
    "\n",
    "    for schema in schema_list:\n",
    "        schema_name = schema\n",
    "        db_path = filepath + str(schema) + filetype\n",
    "\n",
    "        table_list = get_table_names(db_path)\n",
    "        for table in table_list:\n",
    "            table_name = table\n",
    "            column_data = get_column_info(db_path, schema_name, table_name)\n",
    "            schema_data.append(column_data)\n",
    "    \n",
    "    schema_df = (pd.DataFrame(schema_data, columns=['schema','table','column_info']).explode('column_info', ignore_index=True))\n",
    "\n",
    "    schema_df[['c_id','c_name','c_type','notnull','dflt_value','is_pk']] = schema_df.column_info.tolist()\n",
    "\n",
    "    schema_df.drop(columns=['column_info','notnull','dflt_value','is_pk'], inplace=True)\n",
    "\n",
    "    schema_df_processed = df_text_processing(schema_df)\n",
    "    \n",
    "    return schema_df_processed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shubh\\Downloads\\text-to-sql-main\\text-to-sql-main\\notebooks\\1.0_data_wrangling_sqlite.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shubh/Downloads/text-to-sql-main/text-to-sql-main/notebooks/1.0_data_wrangling_sqlite.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m schema_info \u001b[39m=\u001b[39m build_schema_info(\u001b[39m'\u001b[39;49m\u001b[39m../src/data/processed/db/\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m.sqlite\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\shubh\\Downloads\\text-to-sql-main\\text-to-sql-main\\notebooks\\1.0_data_wrangling_sqlite.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shubh/Downloads/text-to-sql-main/text-to-sql-main/notebooks/1.0_data_wrangling_sqlite.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         schema_data\u001b[39m.\u001b[39mappend(column_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shubh/Downloads/text-to-sql-main/text-to-sql-main/notebooks/1.0_data_wrangling_sqlite.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m schema_df \u001b[39m=\u001b[39m (pd\u001b[39m.\u001b[39mDataFrame(schema_data, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mschema\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcolumn_info\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mexplode(\u001b[39m'\u001b[39m\u001b[39mcolumn_info\u001b[39m\u001b[39m'\u001b[39m, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shubh/Downloads/text-to-sql-main/text-to-sql-main/notebooks/1.0_data_wrangling_sqlite.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m schema_df[[\u001b[39m'\u001b[39;49m\u001b[39mc_id\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mc_name\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mc_type\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mnotnull\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mdflt_value\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mis_pk\u001b[39;49m\u001b[39m'\u001b[39;49m]] \u001b[39m=\u001b[39m schema_df\u001b[39m.\u001b[39mcolumn_info\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shubh/Downloads/text-to-sql-main/text-to-sql-main/notebooks/1.0_data_wrangling_sqlite.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m schema_df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcolumn_info\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mnotnull\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdflt_value\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mis_pk\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shubh/Downloads/text-to-sql-main/text-to-sql-main/notebooks/1.0_data_wrangling_sqlite.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m schema_df_processed \u001b[39m=\u001b[39m df_text_processing(schema_df)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3947\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3945\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_frame(key, value)\n\u001b[0;32m   3946\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, (Series, np\u001b[39m.\u001b[39mndarray, \u001b[39mlist\u001b[39m, Index)):\n\u001b[1;32m-> 3947\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_array(key, value)\n\u001b[0;32m   3948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m   3949\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4006\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4003\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array(key, value)\n\u001b[0;32m   4005\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4006\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iset_not_inplace(key, value)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4025\u001b[0m, in \u001b[0;36mDataFrame._iset_not_inplace\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4023\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique:\n\u001b[0;32m   4024\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mshape(value)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(key):\n\u001b[1;32m-> 4025\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mColumns must be same length as key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4027\u001b[0m     \u001b[39mfor\u001b[39;00m i, col \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(key):\n\u001b[0;32m   4028\u001b[0m         \u001b[39mself\u001b[39m[col] \u001b[39m=\u001b[39m igetitem(value, i)\n",
      "\u001b[1;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "schema_info = build_schema_info('../src/data/processed/db/', '.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema</th>\n",
       "      <th>table</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_name</th>\n",
       "      <th>c_type</th>\n",
       "      <th>schema_split</th>\n",
       "      <th>table_split</th>\n",
       "      <th>c_name_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>0</td>\n",
       "      <td>Shop_ID</td>\n",
       "      <td>INT</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Shop ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>1</td>\n",
       "      <td>Address</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>2</td>\n",
       "      <td>Num_of_staff</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Num of staff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>3</td>\n",
       "      <td>Score</td>\n",
       "      <td>REAL</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee_shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>4</td>\n",
       "      <td>Open_Year</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>coffee shop</td>\n",
       "      <td>shop</td>\n",
       "      <td>Open Year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        schema table c_id        c_name c_type schema_split table_split  \\\n",
       "0  coffee_shop  shop    0       Shop_ID    INT  coffee shop        shop   \n",
       "1  coffee_shop  shop    1       Address   TEXT  coffee shop        shop   \n",
       "2  coffee_shop  shop    2  Num_of_staff   TEXT  coffee shop        shop   \n",
       "3  coffee_shop  shop    3         Score   REAL  coffee shop        shop   \n",
       "4  coffee_shop  shop    4     Open_Year   TEXT  coffee shop        shop   \n",
       "\n",
       "   c_name_split  \n",
       "0       Shop ID  \n",
       "1       Address  \n",
       "2  Num of staff  \n",
       "3         Score  \n",
       "4     Open Year  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_info.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Schema Info Columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel json file\n",
    "I can see some easier use with some of our future langchain steps with a json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict of each schema-table combo with corresponding column information.\n",
    "schema_json = (schema_info.groupby(['schema', 'schema_split', 'table','table_split']) #breakout each of these columns into a record for schema-table combos\n",
    "       .apply(lambda x: x[['c_id','c_name','c_name_split','c_type']].to_dict('records')) #brekout these into dictionaries under each of the schema-table combos\n",
    "       .reset_index()\n",
    "       .rename(columns={0:'columns'})\n",
    "       .to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schema': 'academic',\n",
       " 'schema_split': 'academic',\n",
       " 'table': 'author',\n",
       " 'table_split': 'author',\n",
       " 'columns': [{'c_id': 0,\n",
       "   'c_name': 'aid',\n",
       "   'c_name_split': 'aid',\n",
       "   'c_type': 'INT'},\n",
       "  {'c_id': 1,\n",
       "   'c_name': 'homepage',\n",
       "   'c_name_split': 'homepage',\n",
       "   'c_type': 'TEXT'},\n",
       "  {'c_id': 2, 'c_name': 'name', 'c_name_split': 'name', 'c_type': 'TEXT'},\n",
       "  {'c_id': 3, 'c_name': 'oid', 'c_name_split': 'oid', 'c_type': 'INT'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to a json file - savings in our interim data folder\n",
    "with open('../data/interim/schema_info.json', 'w') as file:\n",
    "    json.dump(schema_json, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe to .pkl file\n",
    "\n",
    "I already saved to a json, but this will be incase I want to pull in the direct dataframe via a pickle file incase that's easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as pickle file\n",
    "filepath = '../data/interim/schema_info.pkl'\n",
    "schema_info.to_pickle(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
